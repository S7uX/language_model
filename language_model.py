import re
from collections import defaultdict

from arpa import write_arpa_file
from ngram import generate_ngrams


# ===================================================== Text ========================================================= #
def load_text_from_file(file, number_of_sentence_pseudo_words=1):
    with open(file, 'r') as f:
        return preprocess_text(f.read(), number_of_sentence_pseudo_words)


def preprocess_text(text, number_of_sentence_pseudo_words):
    # text preprocessing/adjustment:
    text = text.lower()
    # replace all none alphanumeric characters with spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    # every line is a sentence
    lines = text.splitlines()

    for i in range(number_of_sentence_pseudo_words):
        # add "sequence beginning" and "sequence end" pseudo words
        for i in range(len(lines)):
            # add "sequence beginning" and "sequence end" pseudo words
            lines[i] = '<s> ' + lines[i] + ' </s>'

    return lines


# ===================================================== Text ========================================================= #


# =================================================== n-grams ======================================================== #
def calculate_ngrams_stats(ngrams):
    stat_dict = defaultdict(lambda: {'count': 0, 'value': 'not present'})
    for ngram in ngrams:
        # ngram ‚âî w‚ÇÅ‚Ä¶w‚Çô = w‚ÇÅ:w‚Çô
        if ngram not in stat_dict:
            # stat_dict[ngram]['count'] ‚âî C(w‚ÇÅ‚Ä¶w‚Çô); n-gram count
            stat_dict[ngram] = {'count': 1, 'value': ngram}
        else:
            ngram_stat = stat_dict.get(ngram)
            ngram_stat['count'] += 1
    return stat_dict


def calculate_predecessor_stats(ngrams):
    predecessors = []
    for ngram in ngrams:
        predecessors.append(ngram[:-1])
    predecessor_stats = calculate_ngrams_stats(predecessors)
    return predecessor_stats


# =================================================== n-grams ======================================================== #


def generate_language_model(n, probabilities_function):
    """
    Language model with probabilities calculated from the passed probabilities_function.
    """
    model = []

    for i in range(1, n + 1):
        probabilities = probabilities_function(i)
        model.append(probabilities)
    return model


def generate_mle_language_model(lines, n):
    """
    Language model with maximum likelihood estimation (ùó†ùóüùóò) ngram probabilities.
    """

    def calculate_mle_ngram_probabilities(n):
        ngrams = generate_ngrams(lines, n)
        ngram_stats = list(calculate_ngrams_stats(ngrams).values())
        if n != 1:
            predecessor_stats = calculate_predecessor_stats(ngrams)

        probabilities = dict()
        for ngram_stat in ngram_stats:
            ngram = ngram_stat["value"]
            if n == 1:
                # for unigrams: divisor ‚âî N; count of all words
                divisor = len(ngrams)
            else:
                predecessor = ngram[:-1]
                # for n-grams: divisor ‚âî C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ); count of n-grams with the same beginning as w‚ÇÅ‚Ä¶w‚Çô
                # --> n-gram beginning: w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ
                divisor = predecessor_stats.get(predecessor)["count"]

            # for unigrams: ngram_count ‚âî c·µ¢ = C(w·µ¢); count of word w·µ¢ (unigram) in text
            # for n-grams: ngram_count ‚âî c·µ¢ = C(w‚ÇÅ‚Ä¶w‚Çô); count of n-gram w‚ÇÅ‚Ä¶w‚Çô
            ngram_count = ngram_stat["count"]

            # P‚Çò‚Çó‚Çë(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) = C(w‚ÇÅ‚Ä¶w‚Çô) ‚ß∏ C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) [JURAFSKY 2008, eqn. 3.12 on p. 5]
            probability = ngram_count / divisor
            probabilities[ngram] = {"value": probability}

        return {"unique_ngram_count": len(ngram_stats), "n": n, "dict": probabilities}

    return generate_language_model(n, calculate_mle_ngram_probabilities)


def count_unique_words(lines):
    return len(calculate_ngrams_stats(generate_ngrams(lines, 1)))


def generate_add_k_language_model(lines, n, k):
    """
    MLE Language model with addtitive/add-k smoothing according to [JURAFSKY 2008, p. 16].
    """
    # unique_word_count ‚âî V [JURAFSKY 2008, eqn. 3.21 on p. 14]
    unique_word_count = count_unique_words(lines)

    def calculate_add_k_probabilities(n):
        ngrams = generate_ngrams(lines, n)
        ngram_stats = calculate_ngrams_stats(ngrams).values()  # dic --> list
        if n != 1:
            predecessor_stats = calculate_predecessor_stats(ngrams)

        probabilities = dict()
        for ngram_stat in ngram_stats:
            ngram = ngram_stat["value"]
            if n == 1:
                # word_count ‚âî N
                word_count = len(ngrams)
                divisor = word_count
            else:
                predecessor = ngram[:-1]
                divisor = predecessor_stats.get(predecessor)['count']

            # dividend ‚âî C(w‚ÇÅ‚Ä¶w‚Çô) + k
            dividend = ngram_stat['count'] + k
            # for unigrams: divisor ‚âî N + k‚ãÖV
            # for n-grams: divisor ‚âî C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) + k‚ãÖV [JURAFSKY 2008, Eqn. 3.26 on p. 16]
            divisor += k * unique_word_count
            # probability ‚âî P‚Çä‚Çñ(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) =ÔºªC(w‚ÇÅ‚Ä¶w‚Çô) + kÔºΩ‚ß∏ÔºªC(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) + kVÔºΩ
            probability = dividend / divisor

            probabilities[ngram] = {"value": probability}

        return {"unique_ngram_count": len(ngram_stats), "n": n, "dict": probabilities}

    return generate_language_model(n, calculate_add_k_probabilities)


def count_possible_extensions(ngrams):
    """
    Count number of possible and unique extensions of a history (w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) [WAIBEL 2015, p. 39].
    Or in other words: "for Witten-Bell smoothing, we will need to use the number of unique words that follow the
    history" [CHEN 1998, eqn. 15 on p. 13].
    N‚ÇÅ‚Çä(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ‚Ä¢) ‚âî ÔΩú{w‚Çô : C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅw‚Çô) > 0}ÔΩú     <-- set cardinality
    """

    history_set = set()
    extension_words = dict()
    for ngram in ngrams:
        history = ngram[:-1]

        if history not in history_set:
            history_set.add(history)
            extension_words[history] = set()

        last_word = ngram[-1]
        extension_words[history].add(last_word)

    history_counts = defaultdict(int)
    for history in extension_words.keys():
        history_counts[history] = len(extension_words[history])

    return history_counts


def generate_witten_bell_language_model(lines, n):
    """
    Language model with Witten-Bell smoothing according to [WAIBEL 2015, p. 39].

    Witten-Bell is a recursive interpolation method.
    recursive interpolation [CHEN 1998, eqn. 12 on p. 11]:
    P·µ¢‚Çô‚Çú‚Çë·µ£‚Çö(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) = Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)‚ãÖP‚Çò‚Çó‚Çë(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) +Ôºª1 - Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩ‚ãÖ P·µ¢‚Çô‚Çú‚Çë·µ£‚Çö(w‚Çô|w‚ÇÇ‚Ä¶w‚Çô‚Çã‚ÇÅ)

    "In particular, the n-th-order smoothed model is defined recursively as a linear interpolation between the n-th-order
     maximum likelihood model and the (n-1)-th-order smoothed model as in equation (12)). [...]

     To motivate Witten-Bell smoothing, we can interpret equation (12) as saying: with probability Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) we should
     use the higher-order model, and with probabilityÔºª1 - Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩwe should use the lower-order model. [...], we
     should take the termÔºª1 - Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩto be the probability that a word not observed after the history (w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) in
     the training data occurs after that history."
    [CHEN 1998, p. 13]
    """

    mle_language_model = generate_mle_language_model(lines, n)
    backoff = dict()

    def calculate_witten_bell_probabilities(n):
        ngrams = generate_ngrams(lines, n)
        predecessor_stats = calculate_predecessor_stats(ngrams)
        mle_probabilities = mle_language_model[n - 1]

        if n != 1:
            possible_extensions_counts = count_possible_extensions(ngrams)

        probabilities = dict()
        # mle_probability ‚âî P‚Çò‚Çó‚Çë(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)
        for ngram in mle_probabilities["dict"].keys():
            mle_probability = mle_probabilities["dict"][ngram]
            if n == 1:
                # end recursion at unigram model
                probability = mle_probability['value']
            else:
                history = ngram[:-1]
                history_count = predecessor_stats.get(history)["count"]
                # possible_extensions_count ‚âî N‚ÇÅ‚Çä(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ‚Ä¢): Count of possible (unique) extensions
                # of a history (w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ).
                possible_extensions_count = possible_extensions_counts[history]

                # Witten-Bell interpolation weights wb_lambda ‚âî Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) [CHEN 1998, eqn. 16 on p. 13]:
                # Ôºª1 - Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩ= N‚ÇÅ‚Çä(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ‚Ä¢) ‚ß∏ÔºªN‚ÇÅ‚Çä(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ‚Ä¢) + C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩ
                wb_lambda = -1 * ((possible_extensions_count / (possible_extensions_count + history_count)) - 1)

                backoff_probability = backoff[n - 1][history]
                backoff_probability["backoff-weight"] = (1 - wb_lambda)

                # P‚Çó(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) = Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)‚ãÖP‚Çò‚Çó‚Çë(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) +Ôºª1 - Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ)ÔºΩ‚ãÖ P‚Çó(w‚Çô|w‚ÇÇ‚Ä¶w‚Çô‚Çã‚ÇÅ)
                probability = wb_lambda * mle_probability['value'] + (1 - wb_lambda) * backoff_probability["value"]

            probabilities[ngram] = {'value': probability}

        backoff[n] = probabilities
        return {"unique_ngram_count": mle_probabilities['unique_ngram_count'], "n": n, "dict": probabilities}

    return generate_language_model(n, calculate_witten_bell_probabilities)


def count_continuations(ngrams):
    """
    "The continuation count of a string ¬∑ is the number of unique single word contexts for that string ¬∑."
    [JURAFSKY 2008, p. 21]

    N‚ÇÅ‚Çä(‚Ä¢w‚ÇÇ‚Ä¶w‚Çô) ‚âî ÔΩú{w‚ÇÅ : C(w‚ÇÅ‚Ä¶w‚ÇÇw‚Çô) > 0}ÔΩú [CHEN 1998, p. 17]
    """

    successor_set = set()
    continuation_words = dict()
    for ngram in ngrams:
        successor = ngram[1:]

        if successor not in successor_set:
            successor_set.add(successor)
            continuation_words[successor] = set()

        first_word = ngram[0]
        continuation_words[successor].add(first_word)

    continuation_counts = defaultdict(int)
    for successor in continuation_words.keys():
        continuation_counts[successor] = len(continuation_words[successor])

    return continuation_counts


def generate_kneser_ney_language_model(lines, n):
    """
    Language model with Kneser-Ney smoothing according to [JURAFSKY 2008, p. 19].
    """

    # memorize highest ngram order for the Kneser-Ney count c‚Çñ‚Çô
    highest_ngram_order = n

    words = generate_ngrams(lines, 1)
    unique_words = list(set(words))

    ngrams_dict = dict()
    continuation_counts_dict = dict()
    extension_counts_dict = dict()
    ngram_stats_dict = dict()

    for i in range(1, n + 1):
        ngrams_dict[i] = generate_ngrams(lines, i)

    for i in range(1, n + 1):
        ngram_stats_dict[i] = calculate_ngrams_stats(ngrams_dict[i])

        if i != 1:
            continuation_counts_dict[i] = count_continuations(ngrams_dict[i])
            extension_counts_dict[i] = count_possible_extensions(ngrams_dict[i])

    def kneser_ney_count(ngram):
        """
        kneser_ney_count ‚âî c‚Çñ‚Çô [JURAFSKY 2008, eqn 3.41 on p. 21]
        "the definition of the count c‚Çñ‚Çô(¬∑) depends on whether we are counting the highest-order n-gram being
         interpolated (for example trigram if we are interpolating trigram, bigram, and unigram) or one of the
         lower-order n-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram) [...]

         The continuation count of a string ¬∑ is the number of unique single word contexts for that string ¬∑."
        """

        print("kneser ney count for: " + str(ngram))
        ngram_order = len(ngram)
        if ngram_order == highest_ngram_order:
            print("ngram count")
            return ngram_stats_dict[ngram_order][ngram]["count"]
        else:
            print("continuation count")
            return continuation_counts_dict[ngram_order + 1][ngram]

    backoff = dict()

    def calculate_kneser_ney_probabilities(n):
        ngrams = ngrams_dict[n]
        unique_ngrams = list(set(ngrams))

        # discount ‚âî d [JURAFSKY 2008, p. 20]
        discount = 0.75

        probabilities = dict()

        for ngram in unique_ngrams:
            if n == 1:  # recursion base
                """
                End recursion at zerogram (0th-order) model --> Interpolating unigrams with zerograms.
                
                
                "To end the recursion, we can take the smoothed 1st-order model to be the maximum likelihood 
                 distribution, or we can take the smoothed 0th-order model to be the uniform distribution [1/V], where 
                 the parameter Œµ is the empty string."
                [JURAFSKY 2008, p. 11]
                """

                unique_word_count = len(unique_words)

                # kn_lambda_epsilon ‚âî Œª(œµ) =Ôºªd / Œ£·µ•(C(v))ÔºΩ‚ãÖ |w':C(w') > 0|
                kn_lambda_epsilon = (discount / len(words)) * unique_word_count

                # [JURAFSKY 2008, eqn. 3.42 p. 21]
                # probability ‚âî P‚Çñ‚Çô(w) =Ôºªmax(c‚Çñ‚Çô(w) - d, 0) ‚ß∏ Œ£·µ•(c‚Çñ‚Çô(v))ÔºΩ+ Œª(Œµ) ¬∑ 1/V
                probability = (max([kneser_ney_count(ngram) - discount, 0]) / len(words) +
                               kn_lambda_epsilon / unique_word_count)

                # unk_probability ‚âî Œª(œµ) ‚ß∏ V
                unk_probability = kn_lambda_epsilon / unique_word_count
                probabilities[('<unk>',)] = {"value": unk_probability}

            else:
                history = ngram[:-1]

                # possible_extensions_count ‚âî |w':C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅw') > 0| = N‚ÇÅ‚Çä(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ‚Ä¢)
                possible_extensions_count = extension_counts_dict[n][history]

                # [JURAFSKY 2008, p. 21]
                # kn_lambda ‚âî Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) =Ôºªd / Œ£·µ•(C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅv))ÔºΩ‚ãÖ |w':C(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅw') > 0|
                kn_lambda = ((discount / sum(ngram_stats_dict[len(ngram)][history + v]["count"] for v in unique_words))
                             * possible_extensions_count)

                backoff_probability = backoff[n - 1][history]
                backoff_probability["backoff-weight"] = kn_lambda

                # [JURAFSKY 2008, eqn. 3.40 p. 21]
                # probability ‚âî
                #   P‚Çñ‚Çô(w‚Çô|w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) =Ôºªmax(c‚Çñ‚Çô(w‚ÇÅ‚Ä¶w‚Çô) - d, 0) ‚ß∏ Œ£·µ•(c‚Çñ‚Çô(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅv))ÔºΩ+ Œª(w‚ÇÅ‚Ä¶w‚Çô‚Çã‚ÇÅ) ¬∑ P‚Çñ‚Çô(w‚Çô|w‚ÇÇ‚Ä¶w‚Çô‚Çã‚ÇÅ)
                probability = (max([kneser_ney_count(ngram) - discount, 0]) /
                               sum(kneser_ney_count(ngram[:-1] + v) for v in unique_words) +
                               kn_lambda * backoff_probability["value"])

            probabilities[ngram] = {"value": probability}

        backoff[n] = probabilities
        return {"unique_ngram_count": len(unique_ngrams), "n": n, "dict": probabilities}

    return generate_language_model(n, calculate_kneser_ney_probabilities)


lines = load_text_from_file("sample_text/sample.text")
language_model = generate_kneser_ney_language_model(lines, 3)
write_arpa_file(language_model, "kneser_ney.lm")
